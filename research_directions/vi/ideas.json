[
    {
      "Name": "vision_transformer_cifar",
      "Title": "Vision Transformers for CIFAR-100: Adapting Self-Attention Models to Small-Scale Image Classification",
      "Experiment": "Implement a Vision Transformer (ViT) architecture specifically tailored for the CIFAR-100 dataset. Modify the standard ViT design to work effectively with 32x32 images by adjusting patch size, embedding dimension, and number of layers. Compare the performance of the adapted ViT against traditional CNNs on CIFAR-100. Investigate the impact of different patch sizes, attention heads, and transformer depths on model accuracy and efficiency. Evaluate the model's ability to capture global dependencies in small-scale images and its generalization capabilities across diverse classes.",
      "Interestingness": 9,
      "Feasibility": 7,
      "Novelty": 8,
      "Thought": "We propose adapting Vision Transformers (ViT) for small-scale image classification tasks, specifically the CIFAR-100 dataset. The research problem is to investigate whether self-attention-based models can effectively capture relevant features and relationships in low-resolution images (32x32) across 100 diverse classes.\n\nThe theoretical foundation is based on the Vision Transformer architecture, which has shown remarkable performance on large-scale image datasets. However, its effectiveness on smaller images and datasets like CIFAR-100 is less explored. The key challenge is to adapt the ViT architecture to work well with the limited spatial dimensions of CIFAR images while maintaining its ability to model long-range dependencies.\n\nIn the proposed model architecture, we start by dividing the 32x32 input image into small patches, potentially as small as 2x2 or 4x4 pixels. Each patch is linearly embedded into a lower-dimensional space. We then add positional embeddings to retain spatial information. The embedded patches are processed through a series of transformer encoder blocks, each consisting of multi-head self-attention (MSA) and feed-forward (FFN) layers:\n\n\\[\nz_0 = [x_p^1E; x_p^2E; ...; x_p^NE] + E_{pos}\n\\]\n\\[\nz'_l = MSA(LN(z_{l-1})) + z_{l-1}\n\\]\n\\[\nz_l = FFN(LN(z'_l)) + z'_l\n\\]\n\nwhere \\(x_p^i\\) are the image patches, \\(E\\) is the patch embedding matrix, \\(E_{pos}\\) are the positional embeddings, \\(LN\\) is layer normalization, and \\(l\\) indexes the transformer layers.\n\nThe output of the transformer encoder is passed through a classification head to produce class probabilities:\n\n\\[\ny = softmax(MLP(LN(z_L[0])))\n\\]\n\nwhere \\(z_L[0]\\) represents the class token output from the last transformer layer.\n\nFor training, we use cross-entropy loss:\n\n\\[\n\\mathcal{L} = -\\sum_{i=1}^{100} y_i \\log(\\hat{y}_i)\n\\]\n\nwhere \\(y_i\\) is the true label and \\(\\hat{y}_i\\) is the predicted probability for class \\(i\\). We employ an optimizer like AdamW with a cosine learning rate schedule and weight decay for regularization.\n\nBy adapting Vision Transformers to the CIFAR-100 dataset, we aim to demonstrate the versatility of self-attention mechanisms in computer vision tasks across different scales. This research could provide insights into the effectiveness of transformer architectures for small-scale image classification and potentially lead to hybrid models that combine the strengths of CNNs and transformers for improved performance on diverse image recognition tasks."
    }
]